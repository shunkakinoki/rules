---
alwaysApply: true
---
---
Source: .ruler/changesets.md
---
# /changesets — Automatic changeset management and generation

This command provides intelligent changeset detection and generation for repositories using the Changesets package. It automatically analyzes current changes (git diff, staged files, or PR context) and generates appropriate changeset entries following conventional commit standards.

## Overview

The changeset system intelligently:
- **Detects changesets setup**: Checks for `.changeset/` directory, `@changesets/cli` dependencies, and configuration files
- **Analyzes current context**: Examines git diff, staged files, or GitHub PR content to determine change types
- **Generates changeset entries**: Creates properly formatted changeset files with conventional commit types
- **Maintains consistency**: Ensures changeset messages align with conventional commit standards
- **Supports multiple workflows**: Works with git-based changes, PR-based changes, or manual entry

## Changesets Detection

The command automatically detects changesets usage through multiple indicators:

```bash
# Check for changesets directory
if [ -d ".changeset" ]; then
  CHANGESETS_ENABLED=true
fi

# Check for changesets CLI in dependencies
if grep -q "@changesets/cli" package.json 2>/dev/null; then
  CHANGESETS_ENABLED=true
fi

# Check for changesets configuration
if [ -f ".changeset/config.json" ]; then
  CHANGESETS_ENABLED=true
fi
```

### Detection Logic (Fish Shell)

```fish
# Check for changesets setup
set CHANGESETS_ENABLED false

if test -d .changeset
  set CHANGESETS_ENABLED true
end

if grep -q "@changesets/cli" package.json >/dev/null 2>&1
  set CHANGESETS_ENABLED true
end

if test -f .changeset/config.json
  set CHANGESETS_ENABLED true
end
```

## Prerequisites

### Changesets Installation
If changesets is not detected, install it first:

```bash
# Using npm
npm install --save-dev @changesets/cli @changesets/changelog-github

# Using yarn
yarn add -D @changesets/cli @changesets/changelog-github

# Using pnpm
pnpm add -D @changesets/cli @changesets/changelog-github

# Using bun
bun add -D @changesets/cli @changesets/changelog-github
```

### Configuration Setup
Initialize changesets if not already configured:

```bash
# Initialize changesets (creates .changeset/config.json)
npx @changesets/cli init

# Or with bun
bunx @changesets/cli init
```

### Required Dependencies
- `@changesets/cli` - Core changesets functionality
- `@changesets/changelog-github` - GitHub changelog integration
- Git repository with proper commit history

## Workflow

### 1. Context Analysis
The command analyzes the current context to determine changeset requirements:

```bash
# Analyze current git status
git status --porcelain

# Check for staged changes
git diff --cached --name-only

# Get recent commit messages for context
git log --oneline -5
```

### 2. Change Type Detection
Automatically determines the appropriate changeset type:

```fish
# Function to detect changeset type from changes
function detect_changeset_type
    set staged_files (git diff --cached --name-only)

    # Check for breaking changes (major version bump indicators)
    if string match -q "*BREAKING CHANGE*" (git log --oneline -1)
        echo "major"
        return
    end

    # Check for new features
    if string match -q "feat:*" (git log --oneline -1)
        echo "minor"
        return
    end

    # Check for bug fixes
    if string match -q "fix:*" (git log --oneline -1)
        echo "patch"
        return
    end

    # Default to patch for other changes
    echo "patch"
end
```

### 3. Changeset Generation
Creates changeset entries based on detected changes:

#### Interactive Generation (for manual use)
```bash
# Generate changeset with automatic type detection (interactive)
npx @changesets/cli add

# Or with specific bump type (interactive)
npx @changesets/cli add --type major
npx @changesets/cli add --type minor
npx @changesets/cli add --type patch
```

#### Non-Interactive Generation (for automation)
```bash
# Generate changeset with custom message (non-interactive)
npx @changesets/cli add --message "Add user authentication system

- Implement login and logout components
- Add JWT token management
- Create authentication context
- Update protected routes"

# Generate changeset for specific package with message
npx @changesets/cli add my-package --message "Update dependencies

- Update React to latest version
- Add new dev dependencies"

# Generate patch changeset for all packages
npx @changesets/cli add --message "Fix security vulnerabilities

- Update vulnerable dependencies
- Add security patches" --type patch
```

### 4. PR-Based Changesets
For GitHub PR context, automatically extracts information:

```bash
# Get PR title and body for changeset content
PR_TITLE=$(gh pr view --json title --jq '.title')
PR_BODY=$(gh pr view --json body --jq '.body')

# Generate changeset from PR context
npx @changesets/cli add --message "Update based on PR: $PR_TITLE"
```

## Automatic Changeset Generation

### Git Diff Analysis
Analyzes git diff to determine appropriate changeset content:

```fish
function generate_changeset_from_diff
    # Get staged changes
    set changed_files (git diff --cached --name-only)

    # Analyze file types and changes
    for file in $changed_files
        switch $file
            case "*.md"
                echo "Updated documentation"
            case "package.json"
                echo "Updated dependencies"
            case "src/**/*"
                echo "Modified source code"
            case "tests/**/*"
                echo "Updated tests"
        end
    end
end
```

### PR Context Analysis
Extracts changeset information from GitHub PR:

```fish
function generate_changeset_from_pr
    set pr_title (gh pr view --json title --jq '.title')
    set pr_number (gh pr view --json number --jq '.number')
    set pr_labels (gh pr view --json labels --jq '.labels[].name')

    # Generate changeset message based on PR content
    echo "Update based on PR #$pr_number: $pr_title"

    # Add labels as context
    if contains "breaking-change" $pr_labels
        echo "BREAKING CHANGE: This update includes breaking changes"
    end
end
```

### Manual Changeset Entry
For manual changeset creation with guided prompts:

```bash
# Interactive changeset creation
npx @changesets/cli add

# Answer prompts:
# - Which packages should have a changeset added? (package name)
# - What kind of change is this? (major/minor/patch)
# - Write a summary of the changes
```

### Automatic Package Detection
For automated workflows, detect which packages have changes:

```fish
# Function to detect changed packages
function detect_changed_packages
    set changed_files (git diff --cached --name-only)

    # Extract package names from changed files
    set packages
    for file in $changed_files
        # Check if file is in a package directory
        if string match -q "*/src/*" $file
            set package_name (string replace -r '.*/src/([^/]+).*' '$1' $file)
            if not contains $package_name $packages
                set packages $packages $package_name
            end
        end
    end

    # If no specific packages found, use all packages or main package
    if test (count $packages) -eq 0
        set packages (string split ',' (grep -r '"name"' package.json | head -1 | string replace -r '.*"name"\s*:\s*"([^"]+)".*' '$1'))
    end

    echo $packages
end
```

```bash
# Use in automation
CHANGED_PACKAGES=$(detect_changed_packages)
if [ -n "$CHANGED_PACKAGES" ]; then
  npx @changesets/cli add $CHANGED_PACKAGES --message "Update based on recent changes

- Modified package files
- Updated dependencies and configurations"
fi
```

## Changeset Types and Standards

### Version Bump Types
- **Major**: Breaking changes, API removals, or significant architectural changes
- **Minor**: New features, enhancements, or backward-compatible additions
- **Patch**: Bug fixes, security patches, or minor improvements

### Changeset Message Format
```markdown
---
"package-name": major|minor|patch
---

Summary of changes made

### Technical Details
- Implementation specifics
- Migration notes for breaking changes
- Testing verification steps

### Breaking Changes
- List of breaking changes if major bump
- Migration instructions for users
```

## Best Practices

### Changeset Management
- **Create changesets for every PR**: Each PR should have a corresponding changeset
- **Use conventional commit types**: Align changeset types with commit message conventions
- **Keep messages concise**: Focus on user-facing impact rather than implementation details
- **Include migration notes**: For breaking changes, provide clear upgrade instructions

### Git Workflow Integration
```bash
# Create feature branch
git checkout -b feat/new-feature

# Make changes and stage them
git add .

# Generate changeset based on changes
npx @changesets/cli add

# Commit with conventional message
git commit -m "feat: add new feature"

# Push branch and create PR
git push -u origin feat/new-feature
gh pr create --title "feat: add new feature" --body "..."
```

### Package-Specific Changesets
For monorepos with multiple packages:

```bash
# Add changeset for specific package
npx @changesets/cli add package-name

# Add changeset for multiple packages
npx @changesets/cli add package-a package-b

# Add changeset for all packages (default)
npx @changesets/cli add
```

## Integration with Other Commands

### Commit Integration
Works alongside `/commit-lint` and `/commit-push`:

```bash
# After making changes
git add .

# Generate changeset
npx @changesets/cli add

# Commit with conventional format
git commit -m "feat: add new feature"

# Push changes
git push
```

### PR Integration
Integrates seamlessly with `/pr-create` workflow:

#### Non-Interactive PR Changeset Generation
```bash
# Generate changeset based on PR context (non-interactive)
PR_TITLE=$(gh pr view --json title --jq '.title')
PR_NUMBER=$(gh pr view --json number --jq '.number')

npx @changesets/cli add --message "Update based on PR #$PR_NUMBER: $PR_TITLE

- Implement changes from pull request
- Address review feedback and requirements
- Ensure compatibility with existing functionality"

echo "✅ Changeset generated for PR #$PR_NUMBER"
```

#### Automated Package Detection for PRs
```bash
# Detect changed packages from PR files
PR_FILES=$(gh pr view $PR_NUMBER --json files --jq '.files[].path')
CHANGED_PACKAGES=""

for file in $PR_FILES; do
  if [[ $file =~ ^src/([^/]+)/ ]]; then
    package="${BASH_REMATCH[1]}"
    if [[ ! $CHANGED_PACKAGES =~ $package ]]; then
      CHANGED_PACKAGES="$CHANGED_PACKAGES $package"
    fi
  fi
done

# Generate changeset for detected packages
if [ -n "$CHANGED_PACKAGES" ]; then
  npx @changesets/cli add $CHANGED_PACKAGES --message "PR #$PR_NUMBER: $PR_TITLE

Changes implemented in this pull request:
- Modified package files
- Updated functionality and features
- Maintained backward compatibility"
fi
```

**Automatic Integration**: When using `/pr-create`, changeset generation happens automatically in Step 3 using non-interactive methods if changesets is detected in your repository.

## Advanced Usage

### Batch Changeset Generation
For multiple related changes:

```bash
# Create changeset for current changes
npx @changesets/cli add

# Add additional context
echo "Additional changes made:" >> .changeset/$(ls .changeset/ | tail -1)
```

### Changeset Version Management
```bash
# Check current version status
npx @changesets/cli status

# Preview upcoming version changes
npx @changesets/cli version --dry-run

# Apply version changes
npx @changesets/cli version
```

### Custom Changeset Configuration
```json
{
  "changelog": "@changesets/cli/changelog",
  "commit": "@changesets/cli/commit",
  "linked": [],
  "access": "restricted",
  "baseBranch": "main",
  "updateInternalDependencies": "patch",
  "ignore": []
}
```

## Error Handling

### Common Issues

**Changesets not detected:**
```bash
# Verify changesets installation
npx @changesets/cli --version

# Check for .changeset directory
ls -la .changeset/

# Reinitialize if needed
npx @changesets/cli init --yes
```

**Changeset generation fails:**
```bash
# Check git status
git status

# Ensure changes are staged
git add .

# Retry changeset generation
npx @changesets/cli add
```

**Version conflicts:**
```bash
# Check for conflicting changesets
npx @changesets/cli status

# Resolve conflicts manually or
git checkout HEAD -- .changeset/
npx @changesets/cli add
```

### Troubleshooting Commands
```bash
# Verify changesets setup
npx @changesets/cli --help

# Check current changesets
ls .changeset/

# View changeset content
cat .changeset/$(ls .changeset/ | head -1)

# Validate changeset format
npx @changesets/cli status
```

## CI/CD Integration

### Automated Changeset Validation
```yaml
# .github/workflows/changesets.yml
name: Changesets
on:
  pull_request:
    types: [opened, synchronize]

jobs:
  validate-changesets:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: '18'
      - run: npm ci
      - run: npx @changesets/cli status
```


### Version Publishing
```yaml
# .github/workflows/release.yml
name: Release
on:
  push:
    branches: [main]

jobs:
  release:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: '18'
      - run: npm ci
      - run: npx @changesets/cli version
      - run: npm publish
```

## Examples

### Feature Addition (Interactive)
```bash
# After implementing user authentication
git add src/components/Auth/ src/hooks/useAuth.ts
npx @changesets/cli add --message "Add user authentication system

- Implement login and logout components
- Add JWT token management
- Create authentication context
- Update protected routes"

git commit -m "feat: add user authentication system"
```

### Feature Addition (Non-Interactive)
```bash
# After implementing user authentication (automated workflow)
git add src/components/Auth/ src/hooks/useAuth.ts

# Generate changeset with commit message automatically
COMMIT_MSG=$(git log --format=%B -n 1 | head -1)
npx @changesets/cli add --message "Add user authentication system

$COMMIT_MSG

- Implement login and logout components
- Add JWT token management
- Create authentication context
- Update protected routes"

git commit -m "feat: add user authentication system"
```

### Complete PR Workflow with Changesets
```bash
# 1. Create feature branch and make changes
git checkout -b feat/add-user-auth
git add src/components/Auth/ src/hooks/useAuth.ts

# 2. Generate changeset (non-interactive)
if [ -d ".changeset" ]; then
  COMMIT_MESSAGE=$(git log --format=%B -n 1 | head -1)
  npx @changesets/cli add --message "Add user authentication system

$COMMIT_MESSAGE

- Implement login and logout components
- Add JWT token management
- Create authentication context
- Update protected routes"
fi

# 3. Commit with conventional format
git commit -m "feat: add user authentication system

- Add login form component with validation
- Implement JWT token handling and storage
- Create auth context for global state management
- Add protected route wrapper component

Closes #123"

# 4. Push and create PR
git push -u origin feat/add-user-auth
gh pr create --title "feat: add user authentication system" --body "...
🤖 Generated with <AI NAME>"
```

### Package-Specific Changeset
```bash
# For monorepo with multiple packages
git add src/auth-package/src/ src/ui-package/src/

# Generate changeset for specific packages
npx @changesets/cli add auth-package ui-package --message "Update authentication and UI components

- Add new auth features in auth-package
- Update UI components in ui-package
- Maintain backward compatibility"

git commit -m "feat: update auth and UI packages"
```

### Bug Fix
```bash
# After fixing login validation
git add src/utils/validation.ts
npx @changesets/cli add --type patch --message "Fix login validation error

- Correct email format validation
- Improve password strength checking
- Add proper error messaging"

git commit -m "fix: resolve login validation error"
```

### Breaking Change
```bash
# After API redesign
git add src/api/v2/
npx @changesets/cli add --type major --message "Redesign authentication API

BREAKING CHANGE: The authenticate() method now requires email parameter
instead of username. Update all authentication calls accordingly.

### Migration Guide
- Replace username parameter with email
- Update authentication service calls
- Modify login form components"

git commit -m "feat!: redesign authentication API"
```

## Reference
- Changesets documentation: https://github.com/changesets/changesets
- Conventional commits: https://www.conventionalcommits.org/
- GitHub changelog integration: https://github.com/changesets/changelog-github

---
Source: .ruler/commands.md
---
@commands
- [/changesets](commands/changesets.md) — Automatic changeset management and generation
- [/commit-lint](commands/commit-lint.md) — Commit linting configuration for AI agents
- [/commit-push](commands/commit-push.md) — Conventional commit and push workflow
- [/issue-create](commands/issue-create.md) — Standard Linear issue checklist
- [/pr-create](commands/pr-create.md) — Comprehensive PR creation workflow
- [/pr-label](commands/pr-label.md) — Comprehensive PR Labeling Guide
- [/serena](commands/serena.md) — Token-efficient Serena MCP command for structured app development and problem-solving

---
Source: .ruler/commit-lint.md
---
# /commit-lint — Commit linting configuration for AI agents

This document outlines the commit linting configuration and guidelines that AI agents must follow when committing files.

## Overview

The commit linting system ensures code quality and consistency by automatically running formatting, linting, and commit message validation checks before any commits are made by AI agents. This comprehensive system uses both lefthook pre-commit hooks and commitlint to prevent the introduction of poorly formatted code or improperly formatted commit messages into the repository.

## Installation

### Commitlint Setup

Install commitlint for commit message validation:

```bash
# Install commitlint CLI and conventional config
bun add -D @commitlint/cli @commitlint/config-conventional

# Install lefthook for pre-commit hooks
bun add -D lefthook

# Initialize lefthook hooks
bun run lefthook:install
```

### Required Dependencies

Ensure these packages are installed:
- `@commitlint/cli` - Commit message linting
- `@commitlint/config-conventional` - Conventional commit rules
- `lefthook` - Git hooks management
- `@biomejs/biome` - Code formatting and linting

## Configuration

### Ruler.toml Configuration

The commit linting configuration is defined in `ruler.toml` under the `[commit]` section:

```toml
[commit]
enabled = true
pre_commit_commands = [
  "bun run format",    # Format code with Biome
  "bun run lint",      # Run linting and checks
  "bun run check"      # Run all quality checks
]
```

### Commitlint Configuration

Commit message validation is configured in `.commitlintrc.json`:

```json
{
  "extends": ["@commitlint/config-conventional"],
  "rules": {
    "type-enum": [
      2,
      "always",
      [
        "feat",
        "fix",
        "docs",
        "style",
        "refactor",
        "test",
        "chore",
        "perf",
        "ci",
        "build",
        "revert"
      ]
    ],
    "type-case": [2, "always", "lower"],
    "type-empty": [2, "never"],
    "subject-empty": [2, "never"],
    "subject-full-stop": [2, "never", "."],
    "header-max-length": [2, "always", 72],
    "body-leading-blank": [1, "always"],
    "footer-leading-blank": [1, "always"]
  }
}
```

### Lefthook Configuration

Git hooks are managed through `lefthook.yml` with three main stages:

1. **Pre-commit**: Code formatting and linting
2. **Commit-msg**: Commit message validation
3. **Pre-push**: Comprehensive quality checks

## Pre-commit Commands

### 1. Code Formatting (`bun run format`)
- Uses Biome to format all code files
- Ensures consistent indentation, spacing, and line breaks
- Applies project-specific formatting rules

### 2. Linting (`bun run lint`)
- Runs comprehensive linting checks
- Validates code quality and style
- Checks for potential bugs and issues

### 3. Quality Checks (`bun run check`)
- Runs all automated quality verification
- Ensures ruler rules are applied
- Verifies no uncommitted changes remain

## File Patterns

The linting system targets specific file types to optimize performance:

- **JavaScript/TypeScript**: `*.{js,jsx,ts,tsx}`
- **Configuration Files**: `*.{json,css,md,yml,yaml,toml}`
- **Programming Languages**: `*.py`, `*.rs`, `*.go`, `*.java`
- **Infrastructure**: `Makefile*`, `Dockerfile*`

## Auto-fix Capabilities

The system includes auto-fix commands that attempt to resolve issues automatically:

- **Biome Format**: `bun run biome:format`
  - Automatically formats code
  - Fixes spacing and indentation issues

- **Biome Check**: `bun run biome:check --write`
  - Auto-fixes linting violations where possible
  - Applies safe corrections

## Agent Responsibilities

When contributing code, AI agents must ensure code quality by:

1. **Run Pre-commit Checks**: Execute all pre-commit commands before finalizing changes
2. **Review Auto-fixes**: Verify that auto-fixes have been applied correctly
3. **Manual Fixes**: Address any issues that cannot be auto-fixed
4. **Verify Quality**: Ensure all checks pass before contributing
5. **Test Changes**: Validate that formatting changes don't break functionality
6. **Solo Authorship**: DO NOT include co-authorship in commit messages - commits should be solo-authored
7. **PR Attribution**: Include AI attribution in PR descriptions, not commit messages

## Package Manager Detection

AI agents must automatically detect which package manager is being used and adapt their behavior accordingly:

### Detection Logic

```bash
# Check for package manager
if [ -f "bun.lockb" ] || [ -f "bun.lock" ]; then
  PACKAGE_MANAGER="bun"
elif [ -f "pnpm-lock.yaml" ]; then
  PACKAGE_MANAGER="pnpm"
elif [ -f "yarn.lock" ]; then
  PACKAGE_MANAGER="yarn"
elif [ -f "package-lock.json" ]; then
  PACKAGE_MANAGER="npm"
else
  PACKAGE_MANAGER="npm"  # fallback to npm
fi
```

#### Fish Detection Logic

```fish
# Check for package manager (Fish)
set PACKAGE_MANAGER npm
if test -f bun.lockb -o -f bun.lock
  set PACKAGE_MANAGER bun
else if test -f pnpm-lock.yaml
  set PACKAGE_MANAGER pnpm
else if test -f yarn.lock
  set PACKAGE_MANAGER yarn
else if test -f package-lock.json
  set PACKAGE_MANAGER npm
end
```

### Package Manager Commands

Based on the detected package manager, use these commands:

- **bun**: `bun add`, `bun run`, `bun install`
- **pnpm**: `pnpm add`, `pnpm run`, `pnpm install`
- **yarn**: `yarn add`, `yarn run`, `yarn install`
- **npm**: `npm install`, `npm run`

## Pre-commit Hook Detection

AI agents must automatically detect which pre-commit hook system is being used and adapt their behavior accordingly:

### Detection Logic

```bash
# Check for lefthook
if [ -f ".lefthook.yml" ] || [ -f "lefthook.yml" ]; then
  HOOK_SYSTEM="lefthook"
elif [ -f ".pre-commit-config.yaml" ]; then
  HOOK_SYSTEM="pre-commit"
else
  HOOK_SYSTEM="none"
fi
```

#### Fish Detection Logic

```fish
# Check for lefthook (Fish)
set HOOK_SYSTEM none
if test -f .lefthook.yml -o -f lefthook.yml
  set HOOK_SYSTEM lefthook
else if test -f .pre-commit-config.yaml
  set HOOK_SYSTEM pre-commit
end
```

### Lefthook Detection and Usage

**Files to check:**
- `lefthook.yml` (project root)
- `.lefthook.yml` (project root)

**When lefthook is detected:**
- Use lefthook commands for validation
- Respect lefthook configuration settings
- Allow lefthook to handle pre-commit, commit-msg, and pre-push hooks
- Do not run duplicate linting if lefthook is configured

**Lefthook workflow:**
```bash
# Let lefthook handle the hooks automatically
git add <files>
git commit -m "feat: add new feature"

# lefthook will automatically run:
# - pre-commit: formatting and linting
# - commit-msg: commitlint validation
# - pre-push: comprehensive checks
```

### Pre-commit Detection and Usage

**Files to check:**
- `.pre-commit-config.yaml` (project root)

**When pre-commit is detected:**
- Use pre-commit commands for validation
- Respect pre-commit configuration
- Allow pre-commit to manage all hooks
- Do not interfere with pre-commit's hook management

**Pre-commit workflow:**
```bash
# Let pre-commit handle the hooks automatically
git add <files>
git commit -m "feat: add new feature"

# pre-commit will automatically run configured hooks
```

### No Hook System Detected

**When neither system is detected:**
- Fall back to manual validation
- Run linting commands directly
- Validate commit messages manually
- Provide guidance for setting up hooks

**Manual workflow:**
```bash
# Run manual checks (adapt commands based on detected package manager)
$PACKAGE_MANAGER run lint
$PACKAGE_MANAGER run format
$PACKAGE_MANAGER run check

# Validate commit message format
echo "feat: add new feature" | npx commitlint

# Then commit
git add <files>
git commit -m "feat: add new feature"
```

### Agent Decision Flow

The agent should follow this decision-making process:

```mermaid
graph TD
    A[Agent starts commit process] --> B{Check for lefthook.yml}
    B -->|Found| C[Use lefthook workflow]
    B -->|Not found| D{Check for .pre-commit-config.yaml}
    D -->|Found| E[Use pre-commit workflow]
    D -->|Not found| F[Use manual validation workflow]

    C --> G[Let lefthook handle all hooks]
    E --> H[Let pre-commit handle all hooks]
    F --> I[Run manual checks and validation]

    G --> J[Commit with confidence]
    H --> J
    I --> J
```

### Conflict Resolution

**If both systems are detected:**
- Prioritize lefthook (more project-specific configuration)
- Warn about potential conflicts
- Suggest consolidating to one system

**Detection priority:**
1. lefthook (project-specific, more flexible)
2. pre-commit (standardized, widely adopted)
3. manual validation (fallback)

### Environment-Specific Behavior

**CI/CD Environment:**
- Always use manual validation
- Run all checks explicitly
- Don't rely on git hooks

**Local Development:**
- Use detected hook system
- Fall back to manual if hooks fail
- Provide helpful error messages

### Implementation Example

Here's how an AI agent can implement the detection logic:

```javascript
// Node.js/TypeScript implementation
function detectHookSystem(projectRoot = process.cwd()) {
  const fs = require('fs');
  const path = require('path');

  // Check for lefthook configuration
  const lefthookFiles = ['lefthook.yml', '.lefthook.yml'];
  for (const file of lefthookFiles) {
    if (fs.existsSync(path.join(projectRoot, file))) {
      return {
        system: 'lefthook',
        configFile: file,
        workflow: 'automatic'
      };
    }
  }

  // Check for pre-commit configuration
  const preCommitFile = '.pre-commit-config.yaml';
  if (fs.existsSync(path.join(projectRoot, preCommitFile))) {
    return {
      system: 'pre-commit',
      configFile: preCommitFile,
      workflow: 'automatic'
    };
  }

  // No hook system detected
  return {
    system: 'manual',
    configFile: null,
    workflow: 'manual'
  };
}

// Usage in agent
const hookConfig = detectHookSystem();

switch (hookConfig.system) {
  case 'lefthook':
    console.log(`✅ Lefthook detected (${hookConfig.configFile})`);
    console.log('Using lefthook workflow...');
    break;

  case 'pre-commit':
    console.log(`✅ Pre-commit detected (${hookConfig.configFile})`);
    console.log('Using pre-commit workflow...');
    break;

  default:
    console.log('⚠️  No hook system detected');
    console.log('Using manual validation workflow...');
    break;
}
```

```python
# Python implementation
import os
from pathlib import Path

def detect_hook_system(project_root: str = ".") -> dict:
    """Detect which pre-commit hook system is being used."""

    # Check for lefthook configuration
    lefthook_files = ['lefthook.yml', '.lefthook.yml']
    for file in lefthook_files:
        if os.path.exists(os.path.join(project_root, file)):
            return {
                'system': 'lefthook',
                'config_file': file,
                'workflow': 'automatic'
            }

    # Check for pre-commit configuration
    pre_commit_file = '.pre-commit-config.yaml'
    if os.path.exists(os.path.join(project_root, pre_commit_file)):
        return {
            'system': 'pre-commit',
            'config_file': pre_commit_file,
            'workflow': 'automatic'
        }

    # No hook system detected
    return {
        'system': 'manual',
        'config_file': None,
        'workflow': 'manual'
    }

# Usage
hook_config = detect_hook_system()

if hook_config['system'] == 'lefthook':
    print(f"✅ Lefthook detected ({hook_config['config_file']})")
    print("Using lefthook workflow...")
elif hook_config['system'] == 'pre-commit':
    print(f"✅ Pre-commit detected ({hook_config['config_file']})")
    print("Using pre-commit workflow...")
else:
    print("⚠️  No hook system detected")
    print("Using manual validation workflow...")
```

## Integration with Existing Tools

### Lefthook Integration
The commit linting system works alongside existing lefthook pre-commit hooks:

- **Pre-commit Hook**: Runs formatting on relevant files
- **Pre-push Hook**: Runs linting before pushing
- **Commit Lint**: Additional validation for AI agent commits

### Biome Configuration
The system leverages the existing Biome configuration:

```json
{
  "formatter": {
    "enabled": true,
    "formatWithErrors": true,
    "indentStyle": "space"
  },
  "linter": {
    "enabled": true,
    "rules": {
      "recommended": true
    }
  }
}
```

## Error Handling

### Common Issues
- **Formatting Failures**: Check Biome configuration and file permissions
- **Linting Errors**: Review error messages and fix manually if auto-fix fails
- **Command Not Found**: Ensure bun and Biome are properly installed

### Troubleshooting Steps
1. Verify Biome installation: `bun biome --version`
2. Check configuration: `bun run biome:check`
3. Run manual format: `bun run biome:format`
4. Review git status after fixes

## Quality Standards

### Code Formatting
- Use 2-space indentation (configured in Biome)
- Consistent line endings and encoding
- Proper spacing around operators and keywords

### Linting Rules
- Follow Biome recommended rules
- No unused variables or imports
- Consistent naming conventions
- Proper error handling

### Commit Message Standards
- Use conventional commit format: `feat:`, `fix:`, `refactor:`, `docs:`, `chore:`
- Keep messages under 72 characters
- Include descriptive details for complex changes

## Best Practices

### For AI Agents
- **Always run linting before contributing code**
- **Review auto-fixes for correctness**
- **Test functionality after formatting changes**
- **Use descriptive commit messages**
- **Solo-authored commits only** - DO NOT include co-authorship in commit messages
- **PR attribution only** - Include AI attribution in PR descriptions, not commits

### For Development Workflow
- **Regular updates**: Keep Biome and dependencies updated
- **Configuration review**: Regularly audit linting rules
- **Performance monitoring**: Ensure linting doesn't slow down workflow
- **Documentation updates**: Keep this document current with configuration changes

## Commit Message Standards

### Conventional Commit Format

All commit messages must follow the conventional commit format:

```bash
<type>[optional scope]: <description>

[optional body]

[optional footer]
```

### Commit Types

- `feat`: A new feature
- `fix`: A bug fix
- `docs`: Documentation only changes
- `style`: Changes that do not affect the meaning of the code
- `refactor`: A code change that neither fixes a bug nor adds a feature
- `test`: Adding missing tests or correcting existing tests
- `chore`: Changes to the build process or auxiliary tools
- `perf`: A code change that improves performance
- `ci`: Changes to CI configuration files and scripts
- `build`: Changes that affect the build system or external dependencies
- `revert`: Reverts a previous commit

### Examples

```bash
# Feature commit
feat: add user authentication system

# Bug fix with scope
fix(auth): resolve login validation error

# Documentation update
docs: update API documentation for v2.0

# Refactoring with body
refactor: simplify user model validation logic

- Remove redundant validation checks
- Consolidate error handling
- Improve code readability

# Breaking change
feat!: change authentication API interface

BREAKING CHANGE: The authenticate() method now requires email parameter
```


This commit linting system ensures that all AI agent contributions maintain the same high standards of code quality, formatting, and commit message conventions as human developers, creating a consistent and maintainable codebase.

---
Source: .ruler/commit-push.md
---
# /commit-push — Conventional commit and push workflow

Use this command when committing changes to follow Conventional Commits standards and push with upstream tracking.

## Prerequisites
- Ensure all changes are staged or ready to be staged
- Work from a feature branch (not main)
- Run pre-commit checks before staging files
- Review and apply any auto-fixes

## Quick Workflow
```bash
# Stage all changes
git add .

# Or stage specific files
git add <file1> <file2>

# Commit with conventional format
git commit -m "feat: add new feature description"

# Push with upstream tracking (first push only)
git push -u origin <branch-name>

# Subsequent pushes (no -u needed)
git push
```

## Package Manager Detection

Before running any package manager commands, detect which package manager is being used:

```bash
# Check for package manager
if [ -f "bun.lockb" ] || [ -f "bun.lock" ]; then
  PACKAGE_MANAGER="bun"
elif [ -f "pnpm-lock.yaml" ]; then
  PACKAGE_MANAGER="pnpm"
elif [ -f "yarn.lock" ]; then
  PACKAGE_MANAGER="yarn"
elif [ -f "package-lock.json" ]; then
  PACKAGE_MANAGER="npm"
else
  PACKAGE_MANAGER="npm"  # fallback to npm
fi
```

### Fish Detection Logic

```fish
# Check for package manager (Fish)
set PACKAGE_MANAGER npm
if test -f bun.lockb -o -f bun.lock
  set PACKAGE_MANAGER bun
else if test -f pnpm-lock.yaml
  set PACKAGE_MANAGER pnpm
else if test -f yarn.lock
  set PACKAGE_MANAGER yarn
else if test -f package-lock.json
  set PACKAGE_MANAGER npm
end
```

## Pre-commit Quality Checks

Before committing, ensure code quality by running:

```bash
# Run formatting with Biome (adapt based on detected package manager)
$PACKAGE_MANAGER run format

# Run linting and checks
$PACKAGE_MANAGER run lint

# Run all quality checks
$PACKAGE_MANAGER run check
```

### Auto-fix Capabilities
The system includes auto-fix commands (adapt based on detected package manager):

- **Biome Format**: `$PACKAGE_MANAGER run biome:format` - Automatically formats code
- **Biome Check**: `$PACKAGE_MANAGER run biome:check --write` - Auto-fixes linting violations

### Quality Verification
- **Review auto-fixes** for correctness before committing
- **Test changes** to ensure formatting doesn't break functionality
- **Verify all checks pass** before finalizing the commit

## Conventional Commit Types

- `feat:` - A new feature
- `fix:` - A bug fix
- `docs:` - Documentation only changes
- `style:` - Changes that do not affect the meaning of the code
- `refactor:` - A code change that neither fixes a bug nor adds a feature
- `test:` - Adding missing tests or correcting existing tests
- `chore:` - Changes to the build process or auxiliary tools
- `perf:` - A code change that improves performance
- `ci:` - Changes to CI configuration files and scripts
- `build:` - Changes that affect the build system or external dependencies
- `revert:` - Reverts a previous commit

## Commit Message Format

```bash
<type>[optional scope]: <description>

[optional body]

[optional footer]
```

### Examples

```bash
# Feature commit
feat: add user authentication system

# Bug fix with scope
fix(auth): resolve login validation error

# Documentation update
docs: update API documentation for v2.0

# Breaking change
feat!: change authentication API interface

BREAKING CHANGE: The authenticate() method now requires email parameter
```

## Interactive Commit Helper

For complex changes, use interactive staging:

```bash
# Interactive staging
git add -p

# Or use git gui for visual staging
git gui

# Then commit
git commit -m "feat: implement user dashboard"
```

## Branch Management

```bash
# Create and switch to new feature branch
git checkout -b feature/new-feature

# Push and set upstream (first time)
git push -u origin feature/new-feature

# Check branch status
git status
git branch -v
```

## Best Practices

- **Keep commits atomic**: Each commit should represent one logical change
- **Write clear descriptions**: Explain what changed and why
- **Stay under 72 characters**: For both title and body lines
- **Use present tense**: "Add feature" not "Added feature"
- **Reference issues**: Use `Closes #123` or `Fixes #456` in commit body
- **No co-authors in commits**: Keep commits solo-authored (per project policy)

## Troubleshooting

### Amend last commit
```bash
# Amend commit message
git commit --amend -m "feat: updated commit message"

# Amend and add more changes
git add <new-files>
git commit --amend --no-edit
```

### Undo last commit (keep changes)
```bash
git reset --soft HEAD~1
```

### Undo last commit (discard changes)
```bash
git reset --hard HEAD~1
```

### Fix commit message issues
```bash
# If commit message fails validation
git commit --amend
# Edit the message in your editor
# Save and exit

# Or amend with new message directly
git commit --amend -m "fix: correct validation logic"
```

## Integration with PR Workflow

After committing and pushing:
1. Use `/pr-create` command to create pull request
2. Follow PR creation checklist
3. Request reviewers as needed

## Reference
- Full commit linting rules: `/commit-lint`
- Pre-commit hook detection: `.ruler/commit-lint.md`
- PR creation workflow: `/pr-create`

---
Source: .ruler/issue-create.md
---
# /issue-create — Standard Linear issue checklist

Follow this command to create Linear issues that align with the project's default assignment and labeling rules.

## Default Assignment
- Assign every new issue to `shunkakinoki` immediately after creation.
- Mention additional collaborators in the description instead of reassigning unless ownership genuinely changes.

## Labeling Rules
Apply capitalised labels sourced from `devops/infra/github/src/labels.ts`:
- Bug
- Documentation
- Duplicate
- Enhancement
- Good First Issue
- Help Wanted
- Invalid
- Question
- Wontfix
- Dependabot
- Renovate
- Dependencies
- Automerge

Add the most relevant primary label (e.g., `Enhancement` for features, `Bug` for fixes) and append `Dependencies`, `Automerge`, or automation labels only when they truly apply.

## Issue Body Guidelines
1. **Problem statement** — Describe the user-facing impact or goal.
2. **Acceptance criteria** — Bullet list of conditions that must be met.
3. **Technical notes** — Implementation hints, links to specs, or affected services.
4. **Testing plan** — Outline manual or automated validation steps.

## Tips
- Use templates where available, but still confirm assignment and labels afterwards.
- Cross-link related GitHub issues or PRs inline to give downstream automation the right context.
- Close the issue (or move to Done) only after verifying acceptance criteria and closing PRs that reference it.

---
Source: .ruler/pr-create.md
---
# /pr-create — Comprehensive PR creation workflow

## OBJECTIVE

This command provides a standardized, comprehensive workflow for creating GitHub Pull Requests with proper formatting, labeling, and quality assurance. The workflow ensures:

- Consistent PR structure and content standards
- Automated quality checks and validation
- Proper conventional commit practices
- Strategic labeling and reviewer assignment
- Comprehensive testing and documentation requirements
- Automatic changeset generation for version management

**CRITICAL RULE**: This command is STRICTLY for creating GitHub Pull Requests only. Creating new script files, executables, or automation tools that replicate or extend this functionality is EXPLICITLY PROHIBITED. All PR creation must go through the established `gh pr create` workflow documented here.

Use this command when preparing a pull request. Follow each section before running `gh pr create`.

## Prerequisites

### GitHub CLI Setup
- **Install GitHub CLI**: `which gh` (install if not found)
- **Verify authentication**: `gh auth status`
- **Required token scope**: Token must have `repo` scope for PR creation
- **Authentication**: Run `gh auth login` if not authenticated

### Branch Requirements
- **Work on feature branches** (never commit directly to `main`)
- **Keep branches focused** on single features/fixes
- **Use descriptive names** (e.g., `fix-build-dependencies`, `feat-user-auth`)
- **Current branch check**: `git branch --show-current`

## Complete Workflow

### Step 1: Prepare Changes
```bash
# Check current status and see what files changed
git status

# Stage specific files or all changes
git add <files>          # Stage specific files
git add .               # Stage all changes in current directory

# Or use interactive staging for complex changes
git add -p              # Interactive patch staging
```

### Step 2: Create Feature Branch (if needed)
```bash
# Create and switch to new feature branch
git checkout -b feature/new-feature-name

# Or switch to existing feature branch
git checkout existing-feature-branch

# Verify you're on the correct branch
git branch --show-current
```

### Step 3: Generate Changeset (if applicable)
```bash
# Check if repository uses changesets and generate changeset automatically
if [ -d ".changeset" ] || grep -q "@changesets/cli" package.json 2>/dev/null || [ -f ".changeset/config.json" ]; then
  echo "🔄 Changesets detected - generating changeset entry..."

  # Generate changeset based on current changes
  npx @changesets/cli add

  echo "✅ Changeset generated successfully"
else
  echo "ℹ️  No changesets setup detected - skipping changeset generation"
fi
```

### Step 4: Commit Changes
```bash
# Commit with conventional commit format
git commit -m "feat: add user authentication system

- Add login form component
- Implement JWT token handling
- Add user session management

Closes #123"

# Or use /commit-and-push command for guided workflow
# Reference: /commit-and-push
```

### Step 5: Push Branch
```bash
# Push and set upstream (first time only)
git push -u origin feature-branch-name

# Subsequent pushes (no -u needed)
git push
```

### Step 6: Create PR via CLI
```bash
gh pr create \
  --title "feat: add user authentication system" \
  --body "## Changes Made
- Added login form component with validation
- Implemented JWT token handling and storage
- Added user session management utilities
- Updated routing to protect authenticated routes

## Technical Details
- Uses React hooks for state management
- Implements secure token storage in localStorage
- Adds middleware for route protection
- Follows existing component patterns and styling

## Testing
- Verified login/logout flow works correctly
- All pre-commit hooks pass (Biome formatting, linting)
- Component tests added for auth utilities
- Manual testing completed on all major browsers
- No breaking changes to existing functionality

🤖 Generated with <AI_TOOL> on <AI_MODEL>" \
  --base main \
  --head feature-branch-name
```

### Step 7: Reviews and (Optional) Auto-merge
Do not merge immediately after creating a PR. First request reviews and wait for required checks to pass. If your repository policy allows it, you may enable auto-merge so GitHub merges the PR once approvals and checks are satisfied.

Variant: `/pr-create auto` — This variant configures the created PR to auto-merge using squash. It never merges immediately; it will merge only after all required approvals and checks pass according to repository rules.

```bash
# (Optional) Enable auto-merge with squash; merges later when ready
gh pr merge <pr-number> --squash --auto
```

**Note**: Auto-squashing should only be used when:
- The PR contains multiple small commits that would benefit from consolidation
- All commits in the PR are related to the same feature/fix
- The commit history doesn't contain important intermediate states that need preservation
- Team policy allows squashing (consult repository guidelines)

## Auto-Merge Variant Workflow

### When to Use `/pr-create auto`
Use this variant when:
- **Repository policy allows auto-merge** with required approvals
- **PR contains multiple related commits** that should be squashed
- **All required checks pass** and approvals are expected
- **You want to reduce manual merge operations**

### Auto-Merge Prerequisites
- **Branch protection rules** must allow auto-merge
- **Required status checks** must be configured
- **Minimum approval requirements** must be met
- **Repository settings** must enable auto-merge

### Step 7 (Auto Variant): Create PR with Auto-Merge
```bash
gh pr create \
  --title "feat: add user authentication system" \
  --body "## Changes Made
- Added login form component with validation
- Implemented JWT token handling and storage
- Added user session management utilities
- Updated routing to protect authenticated routes

## Technical Details
- Uses React hooks for state management
- Implements secure token storage in localStorage
- Adds middleware for route protection
- Follows existing component patterns and styling

## Testing
- Verified login/logout flow works correctly
- All pre-commit hooks pass (Biome formatting, linting)
- Component tests added for auth utilities
- Manual testing completed on all major browsers
- No breaking changes to existing functionality

🤖 Generated with <AI_TOOL> on <AI_MODEL>" \
  --base main \
  --head feature-branch-name
```

### Step 8 (Auto Variant): Enable Auto-Merge
```bash
# Enable auto-merge with squash for the created PR
PR_NUMBER=$(gh pr view --json number --jq '.number')
gh pr merge $PR_NUMBER --squash --auto

echo "✅ Auto-merge enabled for PR #$PR_NUMBER"
echo "🔄 PR will merge automatically when:"
echo "   - All required approvals are received"
echo "   - All status checks pass"
echo "   - Branch protection rules are satisfied"
```

### Auto-Merge Status Monitoring
```bash
# Check auto-merge status
gh pr view <pr-number> --json isInMergeQueue,mergeable,mergeStateStatus

# Monitor merge queue (if using merge queues)
gh pr view <pr-number> --json mergeQueueEntry
```

### Auto-Merge Troubleshooting
```bash
# Check why auto-merge is blocked
gh pr view <pr-number> --json reviewDecision,mergeStateStatus

# View detailed merge requirements
gh pr view <pr-number> --json mergeRequirements

# Manually disable auto-merge if needed
gh pr merge <pr-number> --auto-merge disable
```

### Auto-Merge Best Practices
- **Monitor auto-merge status** regularly until merged
- **Review merge requirements** before enabling auto-merge
- **Test the workflow** on non-critical PRs first
- **Have rollback plan** if auto-merge causes issues
- **Document auto-merge usage** in team guidelines

## PR Content Standards

### Title Format
- **Use conventional commit format**: `feat:`, `fix:`, `refactor:`, `docs:`, `chore:`
- **Keep under 72 characters** for optimal display
- **Be specific and actionable**: "Add user auth" vs "Update stuff"
- **Include scope if relevant**: `fix(auth): resolve login issue`

### Description Requirements
- **Changes Made**: Bullet list of what was modified
- **Technical Details**: Implementation specifics and rationale
- **Testing**: Verification steps and pre-commit status
- **AI Attribution**: `🤖 Generated with <AI_TOOL> on <AI_MODEL>` (current assistant)

### AI Attribution Guidelines
- **Current assistant**: Replace `<AI_TOOL>` with your AI tool name (e.g., Cursor, VS Code with Copilot, JetBrains AI) and `<AI_MODEL>` with your AI model name (e.g., Claude, GPT-4)
- **Format**: `🤖 Generated with <AI_TOOL> on <AI_MODEL>` (no co-authorship)
- **Placement**: At the end of PR description
- **Consistency**: Use same attribution across all generated content

## Step 9: Labeling & Reviewers

### Automatic Labeling
After PR creation, add appropriate labels:
```bash
# Feature PR
gh pr edit <number> --add-label enhancement

# Bug fix PR
gh pr edit <number> --add-label bug

# Documentation PR
gh pr edit <number> --add-label documentation

# Dependencies PR
gh pr edit <number> --add-label dependencies
```

### Request Reviewers
```bash
# Add specific reviewers
gh pr edit <number> --add-reviewer username1,username2

# Add team reviewers
gh pr edit <number> --add-reviewer "@org/team-name"
```

## Error Handling

### Common Issues & Solutions
- **Authentication failed**: Run `gh auth login`
- **Branch not found**: Ensure branch is pushed to remote first
- **Permission denied**: Check repository access and token scopes
- **PR already exists**: Use `gh pr edit` to modify existing PR

### Troubleshooting Commands
```bash
# Check PR status
gh pr list

# View PR details
gh pr view <number>

# Update PR description
gh pr edit <number> --body "Updated description"

# Change PR title
gh pr edit <number> --title "Updated title"

# Close PR
gh pr close <number>
```

## Step 10: Best Practices

### Commit Guidelines
- **Solo-authored commits only** - DO NOT include co-authorship in commit messages
- **NO co-authorship** - Never add "Co-Authored-By: Claude" or similar co-authorship attribution in commits
- **AI name belongs in PR description only** - Use `🤖 Generated with <AI_TOOL> on <AI_MODEL>` format in PR body, not commit messages
- **Use present tense** in commit messages ("Add feature" not "Added feature")
- **Keep commits atomic** and focused on single changes
- **Reference issues** when applicable (`Closes #123`, `Fixes #456`)

### PR Size Management
- **Keep PRs focused** on single concerns or related changes
- **Split large changes** into multiple smaller PRs
- **Use draft PRs** for work-in-progress or incomplete features
- **Request reviews early** for complex changes to get feedback

### Quality Assurance
- **Ensure all pre-commit hooks pass** before creating PR
- **Run tests** before creating PR (adapt commands based on detected package manager)
- **Verify code formatting** (Biome handles this automatically)
- **Check for breaking changes** and document them clearly
- **Test manually** for UI/UX changes

#### Package Manager Detection for Testing

Before running tests, detect which package manager is being used:

```bash
# Check for package manager
if [ -f "bun.lockb" ] || [ -f "bun.lock" ]; then
  PACKAGE_MANAGER="bun"
elif [ -f "pnpm-lock.yaml" ]; then
  PACKAGE_MANAGER="pnpm"
elif [ -f "yarn.lock" ]; then
  PACKAGE_MANAGER="yarn"
elif [ -f "package-lock.json" ]; then
  PACKAGE_MANAGER="npm"
else
  PACKAGE_MANAGER="npm"  # fallback to npm
fi

# Run tests with detected package manager
$PACKAGE_MANAGER run test
```

### CI/CD Integration
- **PRs automatically trigger** CI pipelines
- **Address failing checks** promptly (tests, linting, formatting)
- **Monitor CI status** and fix issues before requesting review
- **Document known limitations** if any checks are expected to fail

## Complete Example

```bash
# 1. Create feature branch
git checkout -b feat-add-user-auth

# 2. Make changes and stage
git add src/components/Auth/ src/utils/auth.ts
git status

# 3. Generate changeset (if changesets is configured)
if [ -d ".changeset" ] || grep -q "@changesets/cli" package.json 2>/dev/null || [ -f ".changeset/config.json" ]; then
  echo "🔄 Generating changeset..."
  npx @changesets/cli add
fi

# 4. Commit with conventional format with no co-authorship
git commit -m "feat: implement user authentication system

- Add login/logout components with form validation
- Implement JWT token management and secure storage
- Create auth context for global state management
- Add protected route wrapper component

Closes #123"

# 5. Push to remote
git push -u origin feat-add-user-auth

# 6. Create PR with comprehensive description
gh pr create \
  --title "feat: implement user authentication system" \
  --body "## Changes Made
- Added complete user authentication flow with login/logout
- Implemented secure JWT token handling and storage
- Created React context for global auth state management
- Added protected route components for authenticated pages

## Technical Details
- Uses React hooks and context for state management
- Implements secure token storage with expiration handling
- Follows existing component architecture and styling patterns
- Includes proper error handling and loading states

## Testing
- All pre-commit hooks pass (Biome formatting, linting, TypeScript checks)
- Component tests added for auth utilities and context
- Manual testing completed for login/logout flows
- Verified compatibility with existing user management

🤖 Generated with <AI_TOOL> on <AI_MODEL>" \
  --base main \
  --head feat-add-user-auth

# 7. Enable auto-squashing (if using /pr-create auto)
gh pr merge <pr-number> --squash --auto

# 8. Add labels and reviewers
gh pr edit <pr-number> --add-label enhancement
gh pr edit <pr-number> --add-reviewer "@org/frontend-team"
```

## Integration with Other Commands

- **Changeset management**: Use `/changesets` for automatic changeset generation and management
- **Commit workflow**: Use `/commit-and-push` for guided conventional commits
- **Code quality**: Reference `.ruler/commit-lint.md` for detailed standards
- **PR labeling**: Use `/pr-labeling` for automated label management

## Reference
- Full policy: `.ruler/pr-creation.md` in this repository
- Changeset management: `/changesets`
- Commit standards: `/commit-and-push`
- Labeling automation: `/pr-labeling`

---
Source: .ruler/pr-label.md
---
# /pr-label - Comprehensive PR Labeling Guide

This document outlines the rules and best practices for automatically labeling GitHub Pull Requests (PRs) based on Conventional Commit types and other criteria.

## GitHub PR Labeling Rules

This rule defines how PR labels are applied automatically. It uses the default labels and maps Conventional Commit types to GitHub labels.

## Default Labels (source of truth)

Names are defined as:

- bug
- documentation
- duplicate
- enhancement
- good first issue
- help wanted
- invalid
- question
- wontfix

## Other Labels (additional ones defined for `shunkakinoki` repositories)

- dependabot
- renovate
- dependencies
- automerge
- codex
- refactor

## Label Mapping Rules

- feat → enhancement
- fix → bug
- docs → documentation
- chore(deps), build(deps) → dependencies
- PR authored by dependabot → dependabot
- PR authored by renovate → renovate
- If title contains "[automerge]" or label "automerge" is requested in the description, also add automerge

Notes:
- Only apply one of enhancement/bug/documentation based on the primary Conventional Commit type at the start of the PR title.
- Do not add unrelated labels automatically.

## Usage

- Run the snippet after creating the PR, or integrate it into your local automation.
- Ensure the label names exist in the repository (they are provisioned via Pulumi from `labels.ts`).

---
Source: .ruler/ruler-apply.md
---
# /ruler-apply — Regenerate agent instructions with Ruler

Use this command whenever `.ruler/` files or command documentation changes, or after pulling updates that touch repository rules.

## When to Run
- After editing any file under `.ruler/` (including `commands/` docs referenced from there)
- Before committing changes that rely on regenerated instruction files such as `AGENTS.md`
- After rebasing or pulling main when rules may have changed

## Command Sequence
```fish
# Regenerate agent instruction files
pnpm run ruler:apply

# Double-check that regeneration left the tree clean
pnpm run ruler:check
```

## What to Inspect
- Review `git status` for modified instruction outputs (e.g., `AGENTS.md`, `.cursor/...`).
- If files changed, skim the diff to ensure new content reflects your rule updates.
- Commit regenerated files together with the rule change so other contributors stay in sync.

## Troubleshooting
- If `pnpm run ruler:check` reports a dirty tree, run `git status --short` to see which files still differ.
- For merge conflicts in generated files, resolve them in the source `.ruler/` Markdown first, re-run `/ruler-apply`, then commit the regenerated outputs.
- When `.gitignore` changes unexpectedly, confirm `[gitignore].enabled` in `.ruler/ruler.toml` matches the desired setting before re-running the command.

---
Source: .ruler/serena.md
---
---
allowed-tools: Read, Glob, Grep, Edit, MultiEdit, Write, Bash, TodoWrite, mcp__serena__check_onboarding_performed, mcp__serena__delete_memory, mcp__serena__find_file, mcp__serena__find_referencing_symbols, mcp__serena__find_symbol, mcp__serena__get_symbols_overview, mcp__serena__insert_after_symbol, mcp__serena__insert_before_symbol, mcp__serena__list_dir, mcp__serena__list_memories, mcp__serena__onboarding, mcp__serena__read_memory, mcp__serena__remove_project, mcp__serena__replace_regex, mcp__serena__replace_symbol_body, mcp__serena__restart_language_server, mcp__serena__search_for_pattern, mcp__serena__switch_modes, mcp__serena__think_about_collected_information, mcp__serena__think_about_task_adherence, mcp__serena__think_about_whether_you_are_done, mcp__serena__write_memory, mcp__context7__resolve-library-id, mcp__context7__get-library-docs
description: Token-efficient Serena MCP command for structured app development and problem-solving
---

From: https://zenn.dev/sc30gsw/articles/ff81891959aaef
Thank you to @sc30gsw for the inspiration!

## Quick Reference

```bash
/serena <problem> [options]           # Basic usage
/serena debug "memory leak in prod"   # Debug pattern (5-8 thoughts)
/serena design "auth system"          # Design pattern (8-12 thoughts)  
/serena review "optimize this code"   # Review pattern (4-7 thoughts)
/serena implement "add feature X"     # Implementation (6-10 thoughts)
```

## Options

| Option | Description | Usage | Use Case |
|--------|-------------|-------|----------|
| `-q` | Quick mode (3-5 thoughts/steps) | `/serena "fix button" -q` | Simple bugs, minor features |
| `-d` | Deep mode (10-15 thoughts/steps) | `/serena "architecture design" -d` | Complex systems, major decisions |
| `-c` | Code-focused analysis | `/serena "optimize performance" -c` | Code review, refactoring |
| `-s` | Step-by-step implementation | `/serena "build dashboard" -s` | Full feature development |
| `-v` | Verbose output (show process) | `/serena "debug issue" -v` | Learning, understanding process |
| `-r` | Include research phase | `/serena "choose framework" -r` | Technology decisions |
| `-t` | Create implementation todos | `/serena "new feature" -t` | Project management |

## Usage Patterns

### Basic Usage
```bash
# Simple problem solving
/serena "fix login bug"

# Quick feature implementation  
/serena "add search filter" -q

# Code optimization
/serena "improve load time" -c
```

### Advanced Usage
```bash
# Complex system design with research
/serena "design microservices architecture" -d -r -v

# Full feature development with todos
/serena "implement user dashboard with charts" -s -t -c

# Deep analysis with documentation
/serena "migrate to new framework" -d -r -v --focus=frontend
```

## Context (Auto-gathered)
- Project files: !`find . -maxdepth 2 -name "package.json" -o -name "*.config.*" | head -5 2>/dev/null || echo "No config files"`
- Git status: !`git status --porcelain 2>/dev/null | head -3 || echo "Not git repo"`

## Core Workflow

### 1. Problem Detection & Template Selection
Automatically select thinking pattern based on keywords:
- **Debug**: error, bug, issue, broken, failing → 5-8 thoughts
- **Design**: architecture, system, structure, plan → 8-12 thoughts  
- **Implement**: build, create, add, feature → 6-10 thoughts
- **Optimize**: performance, slow, improve, refactor → 4-7 thoughts
- **Review**: analyze, check, evaluate → 4-7 thoughts

### 2. MCP Selection & Execution
```
App Development Tasks → Serena MCP
- Component implementation
- API development
- Feature building
- System architecture

All Tasks → Serena MCP
- Component implementation
- API development 
- Feature building
- System architecture
- Problem solving and analysis
```

### 3. Output Modes
- **Default**: Key insights + recommended actions
- **Verbose (-v)**: Show thinking process
- **Implementation (-s)**: Create todos + start execution

## Problem-Specific Templates

### Debug Pattern (5-8 thoughts)
1. Symptom analysis & reproduction
2. Error context & environment check  
3. Root cause hypothesis generation
4. Evidence gathering & validation
5. Solution design & risk assessment
6. Implementation plan
7. Verification strategy
8. Prevention measures

### Design Pattern (8-12 thoughts)  
1. Requirements clarification
2. Constraints & assumptions
3. Stakeholder analysis
4. Architecture options generation
5. Option evaluation (pros/cons)
6. Technology selection
7. Design decisions & tradeoffs
8. Implementation phases
9. Risk mitigation
10. Success metrics
11. Validation plan
12. Documentation needs

### Implementation Pattern (6-10 thoughts)
1. Feature specification & scope
2. Technical approach selection
3. Component/module design
4. Dependencies & integration points
5. Implementation sequence
6. Testing strategy
7. Edge case handling
8. Performance considerations
9. Error handling & recovery
10. Deployment & rollback plan

### Review/Optimize Pattern (4-7 thoughts)
1. Current state analysis
2. Bottleneck identification
3. Improvement opportunities
4. Solution options & feasibility
5. Implementation priority
6. Performance impact estimation
7. Validation & monitoring plan

## Advanced Options

**Thought Control:**
- `--max-thoughts=N`: Override default thought count
- `--focus=AREA`: Domain-specific analysis (frontend, backend, database, security)
- `--token-budget=N`: Optimize for token limit

**Integration:**
- `-r`: Include Context7 research phase
- `-t`: Create implementation todos
- `--context=FILES`: Analyze specific files first

**Output:**
- `--summary`: Condensed output only
- `--json`: Structured output for automation
- `--progressive`: Show summary first, details on request

## Task Execution

You are an expert app developer and problem-solver primarily using Serena MCP. For each request:

1. **Auto-detect problem type** and select appropriate approach
2. **Use Serena MCP**:
   - **All development tasks**: Use Serena MCP tools (https://github.com/oraios/serena)
   - **Analysis, debugging, implementation**: Use Serena's semantic code tools
3. **Execute structured approach** with chosen MCP
4. **Research relevant docs** with Context7 MCP if needed
5. **Synthesize actionable solution** with specific next steps
6. **Create implementation todos** if `-s` flag used

**Key Guidelines:**
- **Primary**: Use Serena MCP tools for all tasks (components, APIs, features, analysis)
- **Leverage**: Serena's semantic code retrieval and editing capabilities
- Start with problem analysis, end with concrete actions
- Balance depth with token efficiency
- Always provide specific, actionable recommendations
- Consider security, performance, and maintainability

**Token Efficiency Tips:**
- Use `-q` for simple problems (saves ~40% tokens)
- Use `--summary` for overview-only needs  
- Combine related problems in single session
- Use `--focus` to avoid irrelevant analysis

---
Source: .ruler/shell-usage.md
---
# Shell Usage Standard (Fish)

This repository uses the Fish shell for interactive commands and automation snippets, aligned with the system configuration. Prefer Fish for new scripts and examples.

## Policy
- Default shell: Fish (`#!/usr/bin/env fish`).
- Provide Fish snippets for automation examples; keep Bash as optional fallback when helpful.
- Keep one-file, shell-agnostic command sequences unchanged when they work in both shells.

## Conventions
- Variables: `set NAME value` (no `=`). Export: `set -x NAME value`.
- Arrays/lists: `set items a b c`; append: `set items $items d`.
- Conditionals: `if test -f file; ...; end`; switches: `switch $var; case value; ...; end`.
- Substitution: `set VAR (command ...)`.
- PATH: `set -x PATH /new/bin $PATH`.

## Shebangs
- Use `#!/usr/bin/env fish` for executable scripts.

## Compatibility Notes
- Avoid `set -euo pipefail`; Fish handles errors differently. Use explicit checks or `status` as needed.
- When porting Bash snippets, verify quoting and list handling.

---
Source: .ruler/tmp.linear-labeling.md
---
# Linear Issue Labeling & Assignment Rules

This rule standardizes Linear issue creation by auto-assigning to `shunkakinoki` and applying capitalized default labels sourced from `devops/infra/github/src/labels.ts`.

## Default Labels (capitalized)

- Bug
- Documentation
- Duplicate
- Enhancement
- Good First Issue
- Help Wanted
- Invalid
- Question
- Wontfix
- Dependabot
- Renovate
- Dependencies
- Automerge

Notes:
- Capitalization follows first-letter uppercase for each word.
- The source `labels.ts` contains lowercase names; apply capitalization at creation time.

## Assignment

- All newly created issues should be assigned to `shunkakinoki` by default.

---
Source: .ruler/tool-search.md
---
# Tool Search Guidelines

This document defines the preferred tools and methodologies for searching and analyzing code within the repository. These tools are optimized for different types of search tasks and should be used according to the patterns outlined below.

## Core Search Tools

### File Discovery
- **Primary**: `fd` - Fast and user-friendly alternative to `find`
  - Search for files by name, extension, or pattern
  - Respects `.gitignore` by default
  - Examples: `fd config.json`, `fd '*.ts'`, `fd -e md`

### Text Search
- **Primary**: `rg` (ripgrep) - High-performance text search
  - Full-text search across files
  - Regular expression support
  - Context options (`-A`, `-B`, `-C` for lines after/before/around matches)
  - Examples: `rg "function.*export"`, `rg -i "todo" -A 3`

### Code Structure Analysis
- **Primary**: `ast-grep` - AST-based code search and analysis
  - **Default to TypeScript contexts** for this codebase
  - Language-specific parsing for accurate structural matches
  - Pattern-based matching using AST nodes rather than text

#### Language Mapping for ast-grep:
- **TypeScript files** (`.ts`): `ast-grep --lang ts -p '<pattern>'`
- **React/TSX files** (`.tsx`): `ast-grep --lang tsx -p '<pattern>'`
- **JavaScript files** (`.js`, `.mjs`): `ast-grep --lang javascript -p '<pattern>'`
- **Rust files** (`.rs`): `ast-grep --lang rust -p '<pattern>'`
- **Python files** (`.py`): `ast-grep --lang python -p '<pattern>'`
- **Go files** (`.go`): `ast-grep --lang go -p '<pattern>'`

#### When to Use ast-grep Over Text Search:
- Finding function definitions, class declarations, or specific code patterns
- Refactoring operations that require understanding code structure
- Type-aware searches (finding usages of specific types, interfaces, etc.)
- Complex code transformations
- Avoiding false positives from comments or strings

#### ast-grep Pattern Examples:
```bash
# Find all function declarations
ast-grep --lang ts -p 'function $NAME($ARGS) { $$$ }'

# Find React components with specific props
ast-grep --lang tsx -p '<$COMP $PROPS>$$$</$COMP>'

# Find class methods
ast-grep --lang ts -p 'class $CLASS { $METHOD($ARGS) { $$$ } }'
```

### Interactive Selection
- **Primary**: `fzf` - Fuzzy finder for interactive selection
  - Pipe search results through fzf for interactive filtering
  - Examples: `fd '*.ts' | fzf`, `rg -l "export" | fzf`

### Data Processing
- **JSON**: `jq` - Command-line JSON processor
  - Parse, filter, and transform JSON data
  - Examples: `cat package.json | jq '.dependencies'`

- **YAML/XML**: `yq` - YAML/XML processor (similar to jq)
  - Handle YAML and XML files with jq-like syntax
  - Examples: `cat config.yaml | yq '.database.host'`

## Tool Selection Decision Tree

### 1. Code Structure Search
- **Use ast-grep when:**
  - Searching for specific code patterns (functions, classes, imports)
  - Need to understand code semantics, not just text matches
  - Working with TypeScript/JavaScript code (most common in this repo)
  - Performing refactoring or code transformation tasks
  - Need to avoid false positives from comments or string literals

### 2. Plain Text Search
- **Use ripgrep (rg) when:**
  - Searching for plain text content (documentation, comments, logs)
  - Simple string matching across multiple files
  - Need contextual lines around matches
  - Working with non-code files (markdown, configuration, etc.)
  - ast-grep doesn't support the target language

### 3. File Discovery
- **Use fd when:**
  - Finding files by name patterns
  - Listing files by extension
  - Exploring directory structures
  - Building file lists for further processing

## Best Practices

### Performance Optimization
- **Combine tools efficiently**: `fd '*.ts' | xargs ast-grep --lang ts -p 'pattern'`
- **Use appropriate scoping**: Limit searches to relevant directories when possible
- **Leverage caching**: Tools like `fd` and `rg` are already optimized for speed

### Accuracy and Precision
- **Prefer structural over textual**: Use ast-grep for code analysis, rg for content search
- **Validate language detection**: Ensure ast-grep uses the correct `--lang` parameter
- **Test patterns incrementally**: Start with simple patterns and refine as needed

### Integration Workflows
```bash
# Example: Find all TypeScript files with export statements, then select interactively
fd '*.ts' | xargs ast-grep --lang ts -l -p 'export $$$' | fzf

# Example: Search for TODO comments and show context
rg -i "todo|fixme" -C 2 --type typescript

# Example: Find and process configuration files
fd 'config.*\.json' | xargs jq '.version' | sort -u
```

### Error Handling
- **Fallback strategy**: If ast-grep fails or language is unsupported, fall back to rg
- **Validate tool availability**: Check tool installation before usage
- **Handle edge cases**: Account for mixed-language files and unusual extensions

## Tool Installation Verification

Before using these tools in scripts or automation:

```bash
# Check tool availability
command -v fd >/dev/null 2>&1 || { echo "fd not installed"; exit 1; }
command -v rg >/dev/null 2>&1 || { echo "ripgrep not installed"; exit 1; }
command -v ast-grep >/dev/null 2>&1 || { echo "ast-grep not installed"; exit 1; }
command -v fzf >/dev/null 2>&1 || { echo "fzf not installed"; exit 1; }
```

## Additional Tools

While the core tools above cover most use cases, these additional tools may be useful for specialized tasks:

- **`grep`**: Fallback for basic text search when rg is unavailable
- **`find`**: Fallback for file discovery when fd is unavailable  
- **`sed`/`awk`**: Text processing and transformation
- **`sort`/`uniq`**: Result deduplication and sorting
- **`xargs`**: Efficient command chaining for large result sets

## Repository-Specific Considerations

Given this repository's focus on TypeScript/JavaScript development:
- **Default to TypeScript context** when using ast-grep
- **Prioritize structural search** for code analysis tasks
- **Use ripgrep for documentation** and configuration file searches
- **Combine tools effectively** for complex search workflows

This tool selection strategy ensures efficient, accurate, and maintainable search operations across the entire codebase.

---
Source: .ruler/workspace-file-references.md
---
# Workspace File Reference Rule

## Overview

This rule governs how workspace files (such as `.code-workspace` files) should reference directories and paths.

## Rule

**DO NOT** refer to the rules directory (`../rules` or similar) in workspace files. The rules directory is for reference only and should not be included as a workspace folder.

Instead, use the current directory path with "." (e.g., `"."`) or reference the appropriate workspace/directory directly.

## Examples

### Incorrect:
```json
{
  "folders": [
    {
      "name": "rules",
      "path": "../rules"
    }
  ]
}
```

### Correct:
```json
{
  "folders": [
    {
      "name": "dotfiles",
      "path": "."
    }
  ]
}
```

## Rationale

The rules directory contains configuration and reference materials that should not be part of the active workspace. Including it can cause confusion and unnecessary clutter in the workspace structure.
